# ğŸ” PromptAudit â€” A Prompt Engineered Framework for AI-Driven Vulnerability Detection
**Author:** Anon -
**Version:** v2.0 (Research Release)

---

## ğŸš€ Overview
**PromptAudit** is an end-to-end research platform for evaluating how prompt engineering techniques influence the ability of **large language models (LLMs)** to classify source code as **SAFE** or **VULNERABLE**.

It provides:

- A polished **GUI (ttkbootstrap style)**  
- Multi-model support (Ollama, HF Transformers, API)  
- Modular prompt strategies (Zero-Shot, Few-Shot, CoT, Adaptive CoT, Selfâ€‘Consistency)  
- Datasets from HuggingFace + local CSV  
- A full **interactive HTML report** (filters, sliders, charts, leaderboard, exports)

This platform is designed for **academic reproducibility**, **benchmarking**, and **prompt-engineering research** in secure software analysis.

---

## ğŸ§© System Pipeline

```
GUI (dashboard.py)
      â”‚
      â–¼
ExperimentRunner (core/runner.py)
      â”‚
      â”œâ”€â”€ Dataset Loader (code_datasets/)
      â”œâ”€â”€ Prompt Strategy (prompts/)
      â”œâ”€â”€ Model Loader (models/)
      â””â”€â”€ Metrics + HTML Report (evaluation/)
```
You select:
- Model
- Prompt technique
- Dataset

PromptAudit automatically:
- Builds prompts
- Sends them to the selected model
- Parses the model output
- Computes metrics
- Generates a multi-section interactive report

---

## ğŸ“¸ Interface Overview

**Figure 1 â€” Main Dashboard**  
![Dashboard Screenshot](docs/screenshots/dashboard_main.png)

**Figure 2 â€” Generation Settings**  
![Generation Settings Screenshot](docs/screenshots/dashboard_config.png)

**Figure 3 â€” Sample HTML Report Output**  
![Report Screenshot](docs/screenshots/report_sample.png)

---

## ğŸ›  System Requirements

| Component       | Minimum               | Recommended                                         |
|-----------------|-----------------------|-----------------------------------------------------|
| RAM             | **8 GB**              | **16â€“32 GB** (Gemma, Mistral, Falcon)               |
| CPU             | Any modern quadâ€‘core  | 8â€‘core+ for faster evaluation                       |
| GPU             | Optional              | **CUDA GPU** strongly recommended for HF models     |
| Disk Space      | 10 GB                 | 30 GB+ (for models & datasets)                      |

**RAM Notes for Ollama Models:**
Recommended 24GB+ RAM
- `mistral` ğŸ”¹ ~8-9 GB RAM  
- `gemma:7b` ğŸ”¹ ~7-8 GB RAM  
- `codellama:7b-instruct` ğŸ”¹ 10-11 GB RAM
- `deepseek-coder:6.7b-instruct` ğŸ”¹ 7-8 GB RAM    
- `falcon:7b-instruct` ğŸ”¹ 12-13 GB RAM  

If memory is insufficient, Ollama returns:  
```bat
Error: model requires more system memory than is available
```

---

## ğŸ’» Installation Guide (Windows 10/11)

### 1ï¸âƒ£ Install Python 3.11+
Download from: https://www.python.org/downloads  
âœ” Check: **â€œAdd Python to PATH.â€**

---

### 2ï¸âƒ£ Create & Activate a Virtual Environment

Create Environment:
```bat
python -m venv venv
```
Activate it:
```bat
venv\Scripts\activate
```

For macOS/Linux:
```bat
source venv/bin/activate
```

Make sure your IDE (PyCharm, VSCode) also points to this **same venv**.

---

### 3ï¸âƒ£ Install Requirements

```bat
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Install Ollama

Download: https://ollama.com/download


After installing, open terminal and run:

```bat
ollama serve
```

Then pull one or more models:

```bat
ollama pull mistral:latest
ollama pull gemma:7b
ollama pull codellama:7b-instruct
ollama pull deepseek-coder:6.7b-instruct
ollama pull falcon:7b-instruct
```

---

### 5ï¸âƒ£ HuggingFace Login (only for HF datasets)

```bat
huggingface-cli login
```

Input your access token. (Check your account profile to create one, readme only)

---

### 6ï¸âƒ£ Launch the run_PromptAudit

```
python run_PromptAudit.py
```

---

ğŸ–¥ï¸ **Using the Interface**

| Area                 | Function |
|----------------------|----------|
| Experiment Info      | Add experiment name and notes (for reports). |
| Models               | Choose installed Ollama models. |
| Prompt Strategy      | Choose the prompting method (Zero-Shot, Few-Shot, CoT, Adaptive CoT, Self-Consistency). |
| Dataset Selector     | Pick datasets (Toy, CVEFixes, BigVul, Vul4J). |
| Generation Settings  | Adjust temperature, max new tokens, top-p, repetition penalty, etc. |
| Run Experiment       | Starts evaluation â†’ live progress updates. |
| Run Panel            | Progress, ETA, live logs (some features not implemented yet). |
| Results Access       | Open interactive HTML report. |

---

## ğŸ“Š Research Output
Generated files:

| File                        | Description                                               |
|-----------------------------|-----------------------------------------------------------|
| `results/report.html`       | Interactive dashboard with filters, leaderboards, charts  |
| `results/csv/metrics.csv`   | Summary metrics                                           |
| `results/logs/*.log`        | Reproducibility logs                                      |
| `results/*.json`            | Raw outputs (optional)                                    |

---

## ğŸ—‚ Project Structure

```
PromptAudit/
â”‚
â”œâ”€â”€ run_PromptAudit.py         # GUI launcher
â”œâ”€â”€ run.bat                    # Windows start script
â”œâ”€â”€ run.sh                     # Linux start script
â”œâ”€â”€ config.yaml                # Models, datasets, generation defaults
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # Project documentation
â”‚
â”œâ”€â”€ code_datasets/
â”‚   â”œâ”€â”€ dataset_loader.py      # Unified loader for local + HF datasets
â”‚   â”œâ”€â”€ toy_dataset.py         # Local toy dataset loader
â”‚   â””â”€â”€ hf_loader.py           # Hugging Face dataset utilities
|
â”œâ”€â”€ core/
â”‚   â””â”€â”€ runner.py              # Experiment orchestration engine
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ toy.csv                # Small built-in dataset
|
â”œâ”€â”€ debug/
â”‚   â””â”€â”€ files.py...            # Test files for debugging
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ screenshots/...        # README images
|
â”œâ”€â”€ evaluation/
|   â”œâ”€â”€ label_parser.py        # Handles the model output responses
â”‚   â”œâ”€â”€ metrics.py             # TP/TN/FP/FN + Accuracy/Precision/Recall/F1
â”‚   â””â”€â”€ report.py              # Full interactive HTML report generator
|
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_loader.py        # Backend selection (Ollama, Dummy, API)
â”‚   â”œâ”€â”€ ollama_model.py        # Local Ollama model wrapper
â”‚   â”œâ”€â”€ api_model.py           # OpenAI-style REST API backend
â”‚   â””â”€â”€ dummy_model.py         # Offline deterministic baseline
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ base_prompt.py         # Core prompt interface
â”‚   â”œâ”€â”€ zero_shot.py           # Zero-shot baseline prompt
â”‚   â”œâ”€â”€ few_shot.py            # Few-shot example prompting
â”‚   â”œâ”€â”€ cot.py                 # Chain-of-Thought prompting
â”‚   â”œâ”€â”€ adaptive_cot.py        # Two-phase CoT fallback logic
â”‚   â”œâ”€â”€ self_consistency.py    # Majority-vote CoT sampling
â”‚   â””â”€â”€ prompt_loader.py       # Prompt strategy registry
|
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ csv/                   # Generated CSV summaries
â”‚   â”œâ”€â”€ logs/                  # Per-run logs
â”‚   â””â”€â”€ report.html            # Generated dashboard
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ dashboard.py           # Full GUI (ttkbootstrap)
â”‚   â””â”€â”€ user_prefs.yaml        # Saved selections & settings
â”‚
â””â”€â”€ utils/
    â””â”€â”€ io.py                  # Directory creation utilities
```

---

## â–¶ï¸ Quick Demo Expirement

1. Launch run_PromptAudit.py  
2. Select:
   - Model: mistral:latest
   - Prompt: zero_shot 
   - Dataset: toy
3. Keep token generation settings defaulted 
4. Add experiment name + notes  
5. Click **Run Experiment**  
6. Wait until the progress bar finishes  
7. Open:  
   ```
   results/report.html
   ```

---

## ğŸ§  Reproducibility

PromptAudit automatically records:
- Model name & version
- Prompt strategy
- Generation parameters
- Dataset name & timestamp
- Experiment name & notes
- Per-sample predictions

The HTML report includes everything needed for citation and reproducibility.

---

## ğŸ§© Troubleshooting

| Symptom                           | Cause                             | Fix                                           |
|-----------------------------------|-----------------------------------|-----------------------------------------------|
| âŒ *No Ollama models detected*    | Server not running                | Run `ollama serve`                            |
| âŒ *500: insufficient memory*     | Model too large                   | Use smaller model (e.g., `mistral:latest`)    |
| âŒ *HF dataset unavailable*       | Not logged in                     | Run `huggingface-cli login`                   |
| ğŸ§Š *Progress bar frozen*          | Large model latency               | Normal â€” wait                                 |
| ğŸ“„ *No report generated*          | Experiment was stopped early      | Reâ€‘run                                        |

---

## ğŸ“˜ Citation (Suggested)

```
ANON.
```
---

ğŸ§Š **Built With**

- ğŸ Python 3.11+
- ğŸ¨ ttkbootstrap (UI framework)
- ğŸ¤— Transformers + Datasets
- ğŸ§® Ollama (local LLMs)
- ğŸ“Š Interactive HTML Reports

---


Â© 2025 Anon â€” All Rights Reserved.
